{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flash_attn"
      ],
      "metadata": {
        "id": "s-C5qTrtTJ8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXLJ0nZAR3u8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForImageTextToText\n",
        "from transformers.image_utils import load_image\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "from torch.profiler import profile, ProfilerActivity, record_function\n",
        "from torch.utils.benchmark import Timer\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load images\n",
        "image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
        "image2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\n",
        "\n",
        "\n",
        "# Initialize processor and model\n",
        "model_id =  \"HuggingFaceTB/SmolVLM-256M-Instruct\"   # \"HuggingFaceTB/SmolVLM-Instruct\"; \"HuggingFaceTB/SmolVLM-500M-Instruct\"; \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.bfloat16,\n",
        "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
        ").to(DEVICE)\n",
        "\n",
        "\n",
        "retain = .5\n",
        "K = int(processor.image_seq_len * retain)\n",
        "processor.image_seq_len = K\n",
        "\n",
        "def prune_visual_tokens_hook(module, inputs, outputs):\n",
        "    # idx = torch.randperm(outputs.shape[1])[:K]\n",
        "    # idx = torch.sort(idx)\n",
        "    # pruned = outputs[:, idx]\n",
        "\n",
        "    # sorted and different per batch\n",
        "    idx = torch.sort(torch.argsort(torch.rand(outputs.shape[0], outputs.shape[1], device=outputs.device), dim=-1)[:,:K], dim=-1).values\n",
        "    pruned = torch.gather(input=outputs, dim=1, index=idx.unsqueeze(-1).expand(-1, -1, outputs.shape[-1]))\n",
        "\n",
        "    return pruned\n",
        "\n",
        "vision_encoder = model.model.connector\n",
        "\n",
        "if retain < 1:\n",
        "  print(\"Attaching Hook\")\n",
        "  handle = vision_encoder.register_forward_hook(prune_visual_tokens_hook)\n",
        "\n",
        "# Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Can you describe the two images?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
        "inputs = inputs.to(DEVICE)\n",
        "\n",
        "# with profile(\n",
        "#     activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True, with_stack=False\n",
        "# ) as prof:\n",
        "#     # Generate outputs\n",
        "#     generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "#     generated_texts = processor.batch_decode(\n",
        "#         generated_ids,\n",
        "#         skip_special_tokens=True,\n",
        "#     )\n",
        "# print(\n",
        "#     prof.key_averages().table(\n",
        "#         sort_by=\"self_cuda_memory_usage\",\n",
        "#         row_limit=20\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "# generated_texts = processor.batch_decode(\n",
        "#     generated_ids,\n",
        "#     skip_special_tokens=True\n",
        "# )\n",
        "# print(generated_texts[0])\n",
        "\n",
        "def generate_outputs():\n",
        "  generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "  generated_texts = processor.batch_decode(\n",
        "      generated_ids,\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "# generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "# torch.cuda.reset_peak_memory_stats()\n",
        "# generated_texts = processor.batch_decode(\n",
        "#       generated_ids,\n",
        "#       skip_special_tokens=True\n",
        "#   )\n",
        "# print(\"Peak allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\\n\")\n",
        "\n",
        "timer = Timer(\n",
        "    stmt='generate_outputs()',\n",
        "    setup='from __main__ import generate_outputs',\n",
        "    num_threads=1\n",
        ")\n",
        "\n",
        "print(timer.timeit(100))\n",
        "\n",
        "\n"
      ]
    }
  ]
}